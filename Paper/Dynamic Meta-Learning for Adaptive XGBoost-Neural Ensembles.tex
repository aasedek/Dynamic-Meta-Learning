\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}

\begin{document}

\title{Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles}

\author{Arthur~Sedek\thanks{A. Sedek is a data scientist at IMDEX Limited mining technology company, Perth, WA, Australia. 
E-mail: arthur.sedek@gmail.com}}

\maketitle

\begin{abstract}
This paper introduces a novel adaptive ensemble framework that synergistically combines XGBoost and neural networks through sophisticated meta-learning. The proposed method leverages advanced uncertainty quantification techniques and feature importance integration to dynamically orchestrate model selection and combination. Experimental results demonstrate superior predictive performance and enhanced interpretability across diverse datasets, contributing to the development of more intelligent and flexible machine learning systems.
\end{abstract}

\section{Introduction}


In the rapidly evolving landscape of machine learning, the quest for models that can adapt to complex, heterogeneous data while maintaining high predictive accuracy remains a significant challenge. Traditional approaches often rely on either tree-based methods, such as XGBoost \cite{chen2016xgboost}, known for their effectiveness with tabular data, or neural networks \cite{lecun2015deep,schmidhuber2015deep}, celebrated for their ability to capture intricate patterns \cite{bishop2006pattern}. However, real-world datasets frequently exhibit characteristics that benefit from both paradigms, necessitating more sophisticated ensemble techniques.

Ensemble methods have long been recognized for their ability to improve predictive performance by combining multiple models \cite{dietterich2000ensemble,caruana2004ensemble,liu2009survey}. Classic approaches like bagging, boosting, and stacking have demonstrated success across various domains \cite{breiman2001random,ho1998random,wolpert1992stacked}. However, these methods typically employ static combination rules, limiting their adaptability to diverse and evolving data distributions \cite{hastie2009ensemble}. Recent advancements in adaptive ensembles have shown promise \cite{cruz2018dynamic}, yet they often focus on homogeneous base models or simplistic combination strategies. Recent advances in machine learning have also emphasized the importance of model interpretability \cite{molnar2019interpretable}. Our approach contributes to this trend by providing insights into model decision-making through feature importance integration and uncertainty quantification \cite{lundberg2017unified,yosinski2014understanding}.

The emergence of meta-learning in recent years has opened new avenues for creating more intelligent ensemble systems \cite{vanschoren2018meta,zhang2021survey}. Meta-learning, or "learning to learn," allows models to improve their learning algorithms over time \cite{finn2017model,snell2017prototypical}, potentially leading to more robust and adaptable systems. While meta-learning has been applied to various aspects of machine learning, its potential in dynamically orchestrating heterogeneous ensembles remains largely unexplored.

In this paper, we present a novel adaptive ensemble framework that synergistically combines the strengths of XGBoost and neural networks through a sophisticated meta-learning approach. Our method goes beyond traditional ensemble techniques by incorporating a dynamic meta-learner that not only selects between models but also learns to identify scenarios where a hybrid approach is optimal. This meta-learner leverages not just the confidence scores of individual models, but also considers feature importances and raw input characteristics, allowing for more nuanced decision-making.

A key innovation in our approach is the integration of advanced uncertainty quantification techniques. For the XGBoost component, we utilize the variance of predictions across trees as a confidence metric, providing insight into the model's certainty for each prediction. On the neural network side, we implement Monte Carlo Dropout \cite{gal2016dropout}, enabling robust uncertainty estimation in deep learning models. These uncertainty measures, combined with feature importance scores derived from both XGBoost and Integrated Gradients \cite{sundararajan2017axiomatic} for neural networks, provide the meta-learner with a rich set of information to guide its decisions.

Our framework addresses several limitations of existing ensemble methods:

\begin{enumerate}
    \item \textbf{Adaptability:} Unlike static ensembles, our system can adjust its prediction strategy on a per-input basis, potentially leading to improved performance across diverse data distributions.
    \item \textbf{Interpretability:} By analyzing the meta-learner's decisions, we gain insights into when and why each base model is preferred, enhancing the interpretability of the overall system.
    \item \textbf{Complementary Strengths:} The combination of XGBoost and neural networks allows our model to handle both structured tabular data and complex, high-dimensional patterns effectively.
    \item \textbf{Uncertainty Awareness:} The incorporation of sophisticated uncertainty quantification techniques enables our model to make more informed decisions and potentially identify out-of-distribution samples.
    \item \textbf{Feature Importance Integration:} By considering feature importances from both models, our meta-learner can make decisions that are sensitive to the varying relevance of features across different parts of the input space.
\end{enumerate}

The proposed method has potential applications across a wide range of domains, including finance, healthcare, and industrial processes, where data complexity and the need for robust, adaptable models are paramount. Our experimental results demonstrate that this approach not only achieves superior predictive performance compared to individual models and static ensembles but also provides valuable insights into model behavior and data characteristics.

To ensure reproducibility and facilitate further research in this area, we have made our complete implementation, including source code and test datasets, publicly available in a GitHub repository \cite{github_repo}. This resource allows other researchers to verify our results, build upon our work, and adapt our approach to their specific use cases.

In the following sections, we provide a detailed description of our methodology, including the architecture of the base models, the design of the meta-learner, and the integration of uncertainty quantification and feature importance techniques. We then present comprehensive experimental results across diverse datasets, analyze the behavior of our adaptive ensemble, and discuss the implications and potential future directions for this line of research.

This work contributes to the growing field of adaptive machine learning systems and opens new avenues for creating more intelligent, flexible, and interpretable predictive models.

\section{Methodology}

Our proposed Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles (DML) combines the strengths of tree-based models and neural networks through a novel meta-learning approach. Our feature importance integration method draws inspiration from recent work in neural network interpretation \cite{shrikumar2017learning,lundberg2017unified}. By combining native XGBoost feature importances with Integrated Gradients \cite{sundararajan2017axiomatic}, we provide a more comprehensive view of feature relevance.This section details the architecture and components of our system.

\subsection{System Architecture}

The DML consists of three main components:
\begin{enumerate}
    \item An XGBoost model
    \item A Neural Network with Monte Carlo Dropout
    \item A Meta-learner
\end{enumerate}

Figure \ref{fig:architecture} illustrates the overall architecture of our system.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.48\textwidth]{architecture.png}}
\caption{Architecture of the Dynamic Meta-Learning XGBoost-Neural Ensemble}
\label{fig:architecture}
\end{figure}

\subsection{XGBoost Component}

We utilize XGBoost as our tree-based model due to its efficiency and effectiveness in handling tabular data. The XGBoost model is defined as:

\begin{equation}
    \hat{y}_i = \phi(x_i) = \sum_{k=1}^K f_k(x_i), \quad f_k \in \mathcal{F}
\end{equation}

where $\mathcal{F}$ is the space of regression trees, $K$ is the number of trees, and $f_k$ represents an independent tree structure with leaf scores.

\subsection{Neural Network with Monte Carlo Dropout}

Our neural network component employs Monte Carlo Dropout for uncertainty estimation. The network architecture is defined as:

\begin{equation}
    \hat{y} = f_\theta(x) = W_L(\text{dropout}(a_{L-1})) + b_L
\end{equation}

where $a_l = \text{dropout}(\text{ReLU}(W_l a_{l-1} + b_l))$ for $l = 1, ..., L-1$, and $a_0 = x$.

During inference, we perform $T$ forward passes with dropout enabled to obtain a distribution of predictions:

\begin{equation}
    \{\hat{y}_t\}_{t=1}^T = \{f_{\theta_t}(x)\}_{t=1}^T
\end{equation}

\subsection{Confidence Metrics}

For XGBoost, we use the variance of predictions across trees as a confidence metric:

\begin{equation}
    c_{xgb} = \text{Var}(\{f_k(x)\}_{k=1}^K)
\end{equation}

For the neural network, we use the variance of Monte Carlo Dropout predictions:

\begin{equation}
    c_{nn} = \text{Var}(\{\hat{y}_t\}_{t=1}^T)
\end{equation}

\subsection{Feature Importance Integration}

We combine feature importances from both models. For XGBoost, we use the built-in feature importance scores. For the neural network, we employ Integrated Gradients:

\begin{equation}
    IG_i(x) = (x_i - x'_i) \times \int_{\alpha=0}^1 \frac{\partial f(x' + \alpha(x-x'))}{\partial x_i} d\alpha
\end{equation}

The combined feature importance for feature $i$ is:

\begin{equation}
    I_i = \lambda \cdot I_{xgb,i} + (1-\lambda) \cdot |IG_i(x)|
\end{equation}

where $\lambda$ is a weighting parameter.

\subsection{Meta-learner}

Our meta-learner is a neural network that takes as input the original features, confidence metrics, and feature importances:

\begin{equation}
    z = [x, c_{xgb}, c_{nn}, I]
\end{equation}

The meta-learner outputs probabilities for selecting XGBoost, Neural Network, or a hybrid prediction:

\begin{equation}
    p = \text{softmax}(W_m \cdot \text{ReLU}(W_h z + b_h) + b_m)
\end{equation}

\subsection{Training Procedure}

The training procedure consists of three phases:

\begin{enumerate}
    \item Train XGBoost and Neural Network independently on the training data.
    \item Generate meta-features (confidences and importances) on a validation set.
    \item Train the meta-learner on the validation set, optimizing for overall prediction accuracy.
\end{enumerate}

The loss function for the meta-learner is:

\begin{equation}
    \mathcal{L} = -\sum_{i=1}^N y_i \log(\hat{y}_i) + \alpha \cdot \text{KL}(p || p_\text{uniform})
\end{equation}

where $\hat{y}_i = p_1 \hat{y}_{xgb,i} + p_2 \hat{y}_{nn,i} + p_3 (\hat{y}_{xgb,i} + \hat{y}_{nn,i})/2$, and the KL divergence term encourages exploration.

\subsection{Inference}

During inference, we:
\begin{enumerate}
    \item Generate predictions and confidence scores from XGBoost and Neural Network.
    \item Compute feature importances.
    \item Use the meta-learner to determine model weights.
    \item Produce the final prediction as a weighted combination of XGBoost and Neural Network outputs.
\end{enumerate}

The final prediction is given by:

\begin{equation}
    \hat{y}_\text{final} = w_{xgb} \hat{y}_{xgb} + w_{nn} \hat{y}_{nn}
\end{equation}

where $w_{xgb}$ and $w_{nn}$ are determined by the meta-learner output.

This adaptive ensemble approach allows our model to leverage the strengths of both XGBoost and Neural Networks, dynamically adjusting its prediction strategy based on the characteristics of each input sample.


\section{Experimental Results}

\subsection{Experimental Setup}

We conducted comprehensive experiments to validate the performance of our Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles (DML) using the California Housing dataset. Our experimental setup includes:

\begin{itemize}
    \item \textbf{Dataset:} California Housing dataset with 20,640 samples and 8 features:
    \begin{itemize}
        \item MedInc (Median Income)
        \item HouseAge (House Age)
        \item AveRooms (Average Rooms)
        \item AveBedrms (Average Bedrooms)
        \item Population
        \item AveOccup (Average Occupancy)
        \item Latitude
        \item Longitude
    \end{itemize}
    Target range: [0.15, 5.00] (in hundreds of thousands of dollars)

    \item \textbf{Data Split:} 80\% training (16,512 samples), 20\% testing (4,128 samples)
    
    \item \textbf{Model Configuration:}
    \begin{itemize}
        \item XGBoost: 150 estimators, learning rate 0.08, max depth 8
        \item Neural Network: [128, 64, 32] hidden units, 150 epochs, dropout 0.3
        \item Meta-learner: [128, 64] hidden units, 100 epochs
        \item Monte Carlo Dropout: 100 samples for uncertainty estimation
    \end{itemize}

    \item \textbf{Baseline Models:} We compared DML against:
    \begin{enumerate}
        \item Individual XGBoost model
        \item Individual Neural Network
        \item Simple average ensemble
    \end{enumerate}

    \item \textbf{Evaluation Metrics:} 
    \begin{itemize}
        \item Root Mean Squared Error (RMSE)
        \item Mean Absolute Error (MAE)
        \item R-squared ($R^2$) score
    \end{itemize}
\end{itemize}

\subsection{Performance Comparison}

The performance of different models on the California Housing dataset is summarized in Table \ref{tab:performance_summary}. The results clearly highlight the superior performance of the DML model, which consistently outperforms other methods across all evaluation metrics.

\begin{table}[htbp]
\centering
\caption{Performance Metrics for Different Models on California Housing Dataset}
\label{tab:performance_summary}
\renewcommand{\arraystretch}{1.2} % Adjusts row height for better readability
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Model} & \textbf{RMSE} & \textbf{MAE} & \textbf{R²} \\
\hline
XGBoost Only        & 0.4718 & 0.3096 & 0.8301 \\
Neural Network Only & 0.5157 & 0.3472 & 0.7970 \\
Simple Average      & 0.4681 & 0.3072 & 0.8328 \\
\textbf{DML}        & \textbf{0.4621} & \textbf{0.3052} & \textbf{0.8370} \\
\hline
\end{tabular}
\end{table}

Our proposed DML demonstrates superior performance compared to individual models and static ensemble methods. Key performance improvements include:

\begin{itemize}
    \item \textbf{2.1\%} improvement in RMSE compared to XGBoost
    \item \textbf{10.4\%} improvement in RMSE compared to Neural Network
    \item \textbf{1.3\%} improvement in RMSE compared to Simple Average
    \item \textbf{4.2\%} improvement in R² compared to Neural Network baseline
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.48\textwidth]{prediction_comparison.png}}
\caption{Prediction Comparison: Actual vs Predicted Values for Different Models}
\label{fig:prediction_comparison}
\end{figure}

Figure \ref{fig:prediction_comparison} provides a visual comparison of the prediction accuracy across different models. The scatter plots demonstrate how closely each model's predictions align with the actual target values, with DML showing the tightest clustering around the diagonal line, indicating superior predictive accuracy.

\subsection{Model Selection Analysis}

Our DML framework demonstrates intelligent model selection behavior through its meta-learner component. The analysis of model selection probabilities on the test set reveals:

\begin{itemize}
    \item \textbf{XGBoost Selection:} Average probability of 0.485 (σ = 0.076)
    \item \textbf{Neural Network Selection:} Average probability of 0.335 (σ = 0.079) 
    \item \textbf{Hybrid Approach:} Average probability of 0.180 (σ = 0.025)
\end{itemize}

The meta-learner shows a preference for XGBoost on this dataset, which aligns with XGBoost's known effectiveness on tabular data. However, the significant probabilities assigned to the neural network and hybrid approaches demonstrate the system's ability to adapt to different input characteristics.

The relatively low standard deviations indicate consistent decision-making patterns, while still maintaining flexibility across different samples. This adaptive behavior represents a key advantage over static ensemble methods.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.48\textwidth]{dml_weight_analysis.png}}
\caption{Model Selection Probability Analysis}
\label{fig:model_selection}
\end{figure}

Figure \ref{fig:model_selection} illustrates the distribution of model selection probabilities, showing how the meta-learner dynamically orchestrates the ensemble components based on input characteristics.

\subsection{Training Efficiency and Computational Analysis}

The DML training process demonstrates reasonable computational efficiency despite its sophisticated architecture:

\begin{itemize}
    \item \textbf{Total Training Time:} 221.73 seconds (3.7 minutes)
    \begin{itemize}
        \item XGBoost training: 0.89 seconds
        \item Neural Network training: 126.49 seconds (57\% of total time)
        \item Meta-feature generation: 90.43 seconds (41\% of total time)
        \item Meta-learner training: 3.90 seconds
    \end{itemize}
    
    \item \textbf{Model Complexity:}
    \begin{itemize}
        \item Neural Network: 11,521 parameters
        \item Meta-learner: 11,523 parameters
        \item Meta-features: 23-dimensional vector per sample
    \end{itemize}
\end{itemize}

The computational overhead is primarily attributed to Monte Carlo Dropout sampling (100 samples) for uncertainty estimation and Integrated Gradients computation for feature importance. This represents a reasonable trade-off for the enhanced performance and interpretability gained.

\section{Discussion}

\subsection{Insights and Implications}

Our adaptive ensemble approach offers several key insights based on the experimental results:

\begin{enumerate}
    \item \textbf{Dynamic Model Selection:} The meta-learner effectively adapts its prediction strategy, showing a clear preference for XGBoost (48.5\% average probability) while maintaining flexibility through neural network (33.5\%) and hybrid (18.0\%) selections based on input characteristics.
    
    \item \textbf{Uncertainty-Aware Predictions:} The integration of Monte Carlo Dropout (100 samples) and tree variance-based confidence metrics enables more reliable uncertainty quantification, contributing to the improved predictive performance.
    
    \item \textbf{Feature Importance Integration:} The 23-dimensional meta-feature vector, incorporating Integrated Gradients and XGBoost feature importances, enables the meta-learner to make informed decisions about model selection.
    
    \item \textbf{Performance Gains:} DML achieves consistent improvements across all metrics, with notable gains in RMSE (2.1\% vs XGBoost, 10.4\% vs Neural Network) and R² scores.
\end{enumerate}

\subsection{Computational Considerations}

Our experimental results provide concrete insights into the computational trade-offs:

\begin{itemize}
    \item \textbf{Training Time:} The total training time of 3.7 minutes is reasonable for the dataset size (16,512 training samples), with neural network training being the most time-consuming component.
    
    \item \textbf{Meta-feature Generation Overhead:} Approximately 41\% of training time is dedicated to generating meta-features, primarily due to Monte Carlo Dropout sampling and Integrated Gradients computation.
    
    \item \textbf{Model Size:} The combined parameter count of approximately 23,000 parameters remains manageable while providing significant performance improvements.
    
    \item \textbf{Scalability:} The approach scales linearly with dataset size, making it suitable for larger datasets with appropriate computational resources.
\end{itemize}

\section{Conclusion}

This paper introduced an innovative dynamic meta-learning framework for adaptive XGBoost-neural ensembles that addresses key limitations in traditional ensemble methods. By leveraging meta-learning, uncertainty quantification, and feature importance integration, we developed a more intelligent and flexible predictive modeling approach.

Our experimental validation on the California Housing dataset demonstrates the effectiveness of the proposed method, achieving consistent improvements across multiple evaluation metrics compared to individual models and static ensemble approaches.

\subsection{Key Contributions}

Our work makes the following contributions:
\begin{itemize}
    \item A novel adaptive ensemble framework with 3-way model selection (XGBoost, Neural Network, or Hybrid)
    \item Implementation of proper Integrated Gradients for neural network feature importance
    \item Advanced uncertainty estimation through Monte Carlo Dropout and tree variance
    \item Demonstrated performance improvements: 2.1\% RMSE improvement over XGBoost, 10.4\% over Neural Network
    \item Comprehensive 23-dimensional meta-feature representation incorporating raw input characteristics
\end{itemize}

\subsection{Future Work}

Potential directions for future research include:
\begin{enumerate}
    \item \textbf{Extended Evaluation:} Validation on additional datasets from different domains to assess generalizability
    \item \textbf{Cross-validation Enhancement:} Implementation of more robust cross-validation strategies for meta-learner training
    \item \textbf{Hyperparameter Optimization:} Automated tuning of the numerous hyperparameters in the ensemble framework
    \item \textbf{Additional Base Models:} Integration of other model types (e.g., Random Forest, Support Vector Machines) into the ensemble
    \item \textbf{Real-time Adaptation:} Development of online learning capabilities for dynamic model updating
    \item \textbf{Computational Optimization:} Investigation of methods to reduce meta-feature generation time while maintaining performance
\end{enumerate}

In conclusion, our research demonstrates the potential of adaptive ensemble methods with sophisticated meta-learning components. The DML framework opens new avenues for creating more intelligent, interpretable, and effective machine learning systems with significant implications for practical applications requiring robust predictive modeling.


\begin{thebibliography}{1}

\bibitem{chen2016xgboost}
T. Chen and C. Guestrin, ``Xgboost: A scalable tree boosting system,'' in \textit{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, 2016, pp. 785--794.

\bibitem{lecun2015deep}
Y. LeCun, Y. Bengio, and G. Hinton, ``Deep learning,'' \textit{Nature}, vol. 521, no. 7553, pp. 436--444, 2015.

\bibitem{dietterich2000ensemble}
T. G. Dietterich, ``Ensemble methods in machine learning,'' in \textit{Multiple Classifier Systems}, 2000, pp. 1--15.

\bibitem{cruz2018dynamic}
R. M. O. Cruz, R. Sabourin, and G. D. C. Cavalcanti, ``A dynamically optimized ensemble of classifiers,'' \textit{Information Sciences}, vol. 424, pp. 108--131, 2018.

\bibitem{vanschoren2018meta}
J. Vanschoren, ``Meta-learning: A survey,'' \textit{arXiv preprint arXiv:1810.03548}, 2018.

\bibitem{gal2016dropout}
Y. Gal and Z. Ghahramani, ``Dropout as a bayesian approximation: Representing model uncertainty in deep learning,'' in \textit{International Conference on Machine Learning}, 2016, pp. 1050--1059.

\bibitem{sundararajan2017axiomatic}
M. Sundararajan, A. Taly, and Q. Yan, ``Axiomatic attribution for deep networks,'' in \textit{International Conference on Machine Learning}, 2017, pp. 3319--3328.

\bibitem{breiman2001random}
L. Breiman, ``Random forests,'' \textit{Machine Learning}, vol. 45, no. 1, pp. 5--32, 2001.

\bibitem{ho1998random}
T. K. Ho, ``The random subspace method for constructing decision forests,'' \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 20, no. 8, pp. 832--844, 1998.

\bibitem{hastie2009ensemble}
T. Hastie, R. Tibshirani, and J. Friedman, ``The Elements of Statistical Learning: Data Mining, Inference, and Prediction,'' Springer, 2009.

\bibitem{caruana2004ensemble}
R. Caruana, ``Ensemble learning,'' in \textit{Proceedings of the International Conference on Machine Learning}, 2004.

\bibitem{liu2009survey}
Y. Liu and Y. Wu, ``A survey of ensemble learning,'' in \textit{Joint European Conference on Machine Learning and Knowledge Discovery in Databases}, 2009, pp. 54--67.

\bibitem{wolpert1992stacked}
D. H. Wolpert, ``Stacked generalization,'' \textit{Neural Networks}, vol. 5, no. 2, pp. 241--259, 1992.

\bibitem{schmidhuber2015deep}
J. Schmidhuber, ``Deep learning in neural networks: An overview,'' \textit{Neural Networks}, vol. 61, pp. 85--117, 2015.

\bibitem{bishop2006pattern}
C. M. Bishop, ``Pattern Recognition and Machine Learning,'' Springer, 2006.

\bibitem{kendall2017uncertainties}
A. Kendall and Y. Gal, ``What uncertainties do we need in Bayesian deep learning for computer vision?'' in \textit{Advances in Neural Information Processing Systems}, 2017, pp. 5574--5584.

\bibitem{molnar2019interpretable}
C. Molnar, ``Interpretable Machine Learning,'' Lulu.com, 2019.

\bibitem{lundberg2017unified}
S. M. Lundberg and S. I. Lee, ``A unified approach to interpreting model predictions,'' in \textit{Advances in Neural Information Processing Systems}, 2017, pp. 4765--4774.

\bibitem{shrikumar2017learning}
A. Shrikumar, P. Greenside, and A. Kundaje, ``Learning important features through propagating activation differences,'' in \textit{International Conference on Machine Learning}, 2017, pp. 3145--3153.

\bibitem{yosinski2014understanding}
J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson, ``Understanding neural networks through deep visualization,'' in \textit{Deep Learning Workshop, International Conference on Machine Learning}, 2015.

\bibitem{zhang2021survey}
X. Zhang, Z. Li, X. Chen, and D. Zhang, ``A survey of meta-learning,'' \textit{IEEE Transactions on Neural Networks and Learning Systems}, 2021.

\bibitem{finn2017model}
C. Finn, P. Abbeel, and S. Levine, ``Model-agnostic meta-learning for fast adaptation of deep networks,'' in \textit{International Conference on Machine Learning}, 2017, pp. 1126--1135.

\bibitem{snell2017prototypical}
J. Snell, K. Swersky, and R. S. Zemel, ``Prototypical networks for few-shot learning,'' in \textit{Advances in Neural Information Processing Systems}, 2017, pp. 4077--4087.

\bibitem{github_repo}
A. Sedek, ``Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles,'' GitHub repository, 
\url{https://github.com/aasedek/Adaptive-XGBoost-Neural-Network-Ensemble}, 2024.

\end{thebibliography}

\end{document}